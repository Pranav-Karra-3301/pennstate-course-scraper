name: Weekly Course Scraping

on:
  schedule:
    # Run every Sunday at 6 AM UTC (1 AM EST, 10 PM PST Sat)
    - cron: '0 6 * * 0'
  workflow_dispatch:  # Allow manual triggering
    inputs:
      test_mode:
        description: 'Run in test mode (limited subjects)'
        required: false
        default: 'false'
        type: boolean
      max_subjects:
        description: 'Maximum subjects to scrape (for testing)'
        required: false
        default: '10'
        type: string

jobs:
  scrape-courses:
    runs-on: ubuntu-latest
    permissions:
      contents: write
      pull-requests: write
    
    steps:
    - name: Checkout repository
      uses: actions/checkout@v4
      with:
        token: ${{ secrets.GITHUB_TOKEN }}
    
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.11'
        cache: 'pip'
    
    - name: Install dependencies
      run: |
        pip install -r requirements.txt
        pip install bc  # For calculations
    
    - name: Create data directory
      run: |
        mkdir -p data
    
    - name: Run course scraper
      id: scrape
      run: |
        echo "Starting course scrape at $(date)"
        
        # Set up variables
        start_time=$(date +%s)
        datestamp=$(date +%Y%m%d)
        output_file="data/psu_courses_${datestamp}.jsonl"
        
        # Determine scraping parameters
        if [ "${{ inputs.test_mode }}" = "true" ]; then
          echo "Running in test mode with ${{ inputs.max_subjects }} subjects"
          max_subjects_arg="--max-subjects ${{ inputs.max_subjects }}"
          workers=5
          detail_workers=10
        else
          echo "Running full scrape"
          max_subjects_arg=""
          workers=12
          detail_workers=40
        fi
        
        # Run the scraper
        echo "Executing scraper..."
        python scraper_optimized.py \
          --output "${output_file}" \
          --format jsonl \
          --campus UP \
          --max-workers ${workers} \
          --max-detail-workers ${detail_workers} \
          --rate-limit 15 \
          --retry-attempts 3 \
          ${max_subjects_arg} 2>&1 | tee scrape_output.log
        
        # Calculate duration
        end_time=$(date +%s)
        duration=$((end_time - start_time))
        
        # Extract statistics from log - updated patterns for optimized scraper
        unique_courses=$(grep -o "Unique courses: [0-9]*" scrape_output.log | grep -o "[0-9]*" | tail -1 || echo "0")
        total_sections=$(grep -o "Total sections: [0-9]*" scrape_output.log | grep -o "[0-9]*" | tail -1 || echo "0")
        subjects_processed=$(grep -o "Subjects processed: [0-9]*/[0-9]*" scrape_output.log | tail -1 | cut -d'/' -f1 || echo "0")
        total_subjects=$(grep -o "Subjects processed: [0-9]*/[0-9]*" scrape_output.log | tail -1 | cut -d'/' -f2 || echo "0")
        failed_subjects=$(grep -o "Failed subjects: [0-9]*" scrape_output.log | grep -o "[0-9]*" | tail -1 || echo "0")
        
        # Calculate rates (avoid division by zero)
        if [ "$duration" -gt 0 ] && [ "$total_sections" -gt 0 ]; then
          sections_rate=$(echo "scale=2; $total_sections / $duration" | bc -l)
        else
          sections_rate="0"
        fi
        
        if [ "$duration" -gt 0 ] && [ "$unique_courses" -gt 0 ]; then
          courses_rate=$(echo "scale=2; $unique_courses / $duration" | bc -l)
        else
          courses_rate="0"
        fi
        
        # Calculate success rate
        if [ "$total_subjects" -gt 0 ]; then
          success_rate=$(echo "scale=1; ($subjects_processed - $failed_subjects) * 100 / $total_subjects" | bc -l)
        else
          success_rate="0"
        fi
        
        # Store outputs for later steps
        echo "duration=$duration" >> $GITHUB_OUTPUT
        echo "unique_courses=$unique_courses" >> $GITHUB_OUTPUT
        echo "total_sections=$total_sections" >> $GITHUB_OUTPUT
        echo "subjects_processed=$subjects_processed" >> $GITHUB_OUTPUT
        echo "total_subjects=$total_subjects" >> $GITHUB_OUTPUT
        echo "failed_subjects=$failed_subjects" >> $GITHUB_OUTPUT
        echo "sections_rate=$sections_rate" >> $GITHUB_OUTPUT
        echo "courses_rate=$courses_rate" >> $GITHUB_OUTPUT
        echo "success_rate=$success_rate" >> $GITHUB_OUTPUT
        echo "filename=psu_courses_${datestamp}.jsonl" >> $GITHUB_OUTPUT
        echo "date=$(date +%Y-%m-%d)" >> $GITHUB_OUTPUT
        echo "datetime=$(date)" >> $GITHUB_OUTPUT
        
        # Verify output file exists and get size
        if [ -f "${output_file}" ]; then
          file_size=$(wc -c < "${output_file}")
          line_count=$(wc -l < "${output_file}")
          echo "file_size=$file_size" >> $GITHUB_OUTPUT
          echo "line_count=$line_count" >> $GITHUB_OUTPUT
          echo "âœ… Output file created: ${output_file} (${line_count} lines, ${file_size} bytes)"
        else
          echo "âŒ Output file not found: ${output_file}"
          echo "file_size=0" >> $GITHUB_OUTPUT
          echo "line_count=0" >> $GITHUB_OUTPUT
          exit 1
        fi
        
        # Summary
        echo "============================================"
        echo "Scraping completed in $duration seconds"
        echo "Unique courses: $unique_courses"
        echo "Total sections: $total_sections"  
        echo "Subjects: $subjects_processed/$total_subjects (${success_rate}% success)"
        echo "Rate: $sections_rate sections/sec, $courses_rate courses/sec"
        echo "============================================"
    
    - name: Validate output
      run: |
        output_file="data/${{ steps.scrape.outputs.filename }}"
        
        if [ ! -f "$output_file" ]; then
          echo "âŒ Output file missing: $output_file"
          exit 1
        fi
        
        # Check if file is valid JSON lines
        echo "ðŸ” Validating JSONL format..."
        python -c "
import json
import sys

try:
    with open('$output_file', 'r') as f:
        lines = f.readlines()
    
    if not lines:
        print('âŒ File is empty')
        sys.exit(1)
    
    valid_lines = 0
    for i, line in enumerate(lines):
        try:
            data = json.loads(line.strip())
            if 'course' in data and 'sections' in data:
                valid_lines += 1
            else:
                print(f'âš ï¸ Line {i+1}: Missing required fields')
        except json.JSONDecodeError as e:
            print(f'âŒ Line {i+1}: Invalid JSON - {e}')
            sys.exit(1)
    
    print(f'âœ… Validation passed: {valid_lines}/{len(lines)} valid course records')
    
    # Sample first record
    first_record = json.loads(lines[0])
    course_info = first_record.get('course', {})
    sections_info = first_record.get('sections', [])
    
    print(f'ðŸ“š Sample course: {course_info.get(\"course_code\", \"N/A\")} - {course_info.get(\"course_title\", \"N/A\")}')
    print(f'ðŸ“ Sections: {len(sections_info)}')
    
except Exception as e:
    print(f'ðŸ’¥ Validation error: {e}')
    sys.exit(1)
        "
        
        echo "âœ… Output validation passed"
    
    - name: Create summary report
      run: |
        cat > scrape_summary.md << 'EOF'
        # Course Scraping Summary - ${{ steps.scrape.outputs.date }}
        
        ## ðŸ“Š Statistics
        
        - **Date**: ${{ steps.scrape.outputs.datetime }}
        - **Duration**: ${{ steps.scrape.outputs.duration }} seconds (${{ steps.scrape.outputs.duration }} / 60 = $((${{ steps.scrape.outputs.duration }} / 60)) minutes)
        - **Unique Courses**: ${{ steps.scrape.outputs.unique_courses }}
        - **Total Sections**: ${{ steps.scrape.outputs.total_sections }}
        - **Subjects Processed**: ${{ steps.scrape.outputs.subjects_processed }}/${{ steps.scrape.outputs.total_subjects }}
        - **Failed Subjects**: ${{ steps.scrape.outputs.failed_subjects }}
        - **Success Rate**: ${{ steps.scrape.outputs.success_rate }}%
        - **Processing Rate**: ${{ steps.scrape.outputs.sections_rate }} sections/second, ${{ steps.scrape.outputs.courses_rate }} courses/second
        
        ## ðŸ“ Output File
        
        - **Filename**: `data/${{ steps.scrape.outputs.filename }}`
        - **Format**: JSONL (optimized structure)
        - **Size**: ${{ steps.scrape.outputs.file_size }} bytes
        - **Records**: ${{ steps.scrape.outputs.line_count }} course records
        - **Campus**: University Park (UP)
        - **Semester**: Fall 2025
        
        ## ðŸ” Data Quality
        
        - **Structure**: Optimized format separating course-level from section-level data
        - **Completeness**: Each record includes comprehensive course and section details
        - **Validation**: All records validated as proper JSON with required fields
        
        ## ðŸ“ Notes
        
        This automated scraping run was executed on GitHub Actions using:
        - Respectful rate limiting (15 requests/second)
        - Comprehensive error handling and retries
        - Optimized data structure to reduce redundancy
        - Focus on University Park campus courses
        
        ## ðŸ¤– Automation Details
        
        - **Triggered**: ${{ github.event_name == 'schedule' && 'Weekly schedule (Sundays 6 AM UTC)' || 'Manual trigger' }}
        - **Environment**: Ubuntu Latest (GitHub Actions)
        - **Python Version**: 3.11
        - **Scraper Version**: Optimized with enhanced data structure
        
        ---
        
        Generated automatically by [Penn State Course Scraper](https://github.com/${{ github.repository }})
        EOF
    
    - name: Configure Git
      run: |
        git config --local user.email "action@github.com"
        git config --local user.name "GitHub Action Bot"
    
    - name: Create Pull Request
      uses: peter-evans/create-pull-request@v5
      with:
        token: ${{ secrets.GITHUB_TOKEN }}
        commit-message: |
          ðŸ“Š Course data update - ${{ steps.scrape.outputs.date }}
          
          - Unique courses: ${{ steps.scrape.outputs.unique_courses }}
          - Total sections: ${{ steps.scrape.outputs.total_sections }}
          - Processing time: ${{ steps.scrape.outputs.duration }}s
          - Success rate: ${{ steps.scrape.outputs.success_rate }}%
        title: "ðŸ“Š Course Data Update - ${{ steps.scrape.outputs.date }}"
        body: |
          ## ðŸŽ“ Penn State Course Data Update
          
          This automated pull request contains fresh course data scraped from Penn State LionPath.
          
          ### ðŸ“ˆ Performance Metrics
          
          | Metric | Value |
          |--------|-------|
          | **Unique Courses** | ${{ steps.scrape.outputs.unique_courses }} |
          | **Total Sections** | ${{ steps.scrape.outputs.total_sections }} |
          | **Processing Time** | ${{ steps.scrape.outputs.duration }}s ($((${{ steps.scrape.outputs.duration }} / 60)) minutes) |
          | **Processing Rate** | ${{ steps.scrape.outputs.courses_rate }} courses/sec, ${{ steps.scrape.outputs.sections_rate }} sections/sec |
          | **Success Rate** | ${{ steps.scrape.outputs.success_rate }}% (${{ steps.scrape.outputs.subjects_processed }}/${{ steps.scrape.outputs.total_subjects }} subjects) |
          | **File Size** | ${{ steps.scrape.outputs.file_size }} bytes |
          
          ### ðŸ“ Files Added/Updated
          
          - `data/${{ steps.scrape.outputs.filename }}` - Course data in optimized JSONL format
          - `scrape_summary.md` - Detailed summary report
          
          ### ðŸ”§ Data Structure
          
          This update uses the **optimized data structure** that separates course-level information from section-specific details:
          
          - **50-70% smaller file sizes** through reduced redundancy
          - **Logical separation** of course vs. section data  
          - **Built-in statistics** for enrollment and capacity aggregation
          - **Comprehensive information** including all course details, schedules, enrollment data, and requirements
          
          ### ðŸŽ¯ Campus Focus
          
          - **Target**: University Park (UP) campus
          - **Filtering**: Smart detection to exclude branch campuses and World Campus
          - **Quality**: Comprehensive course information with detailed section data
          
          ### ðŸ¤– Automation
          
          - **Trigger**: ${{ github.event_name == 'schedule' && 'Weekly automated run' || 'Manual execution' }}
          - **Rate Limiting**: 15 requests/second for respectful scraping
          - **Error Handling**: Comprehensive retry logic and failure tracking
          - **Validation**: Automated JSON validation and data quality checks
          
          ---
          
          **Note**: This is an automated pull request. The data was collected using respectful scraping practices and includes comprehensive course information for academic and research purposes.
        branch: course-update-${{ steps.scrape.outputs.date }}
        delete-branch: true
        labels: |
          automated
          data-update
          course-scraping
    
    - name: Upload scraping logs and artifacts
      if: always()
      uses: actions/upload-artifact@v3
      with:
        name: scraping-logs-${{ steps.scrape.outputs.date }}
        path: |
          scrape_output.log
          psu_scraper_optimized.log
        retention-days: 30
    
    - name: Upload course data artifact
      if: success()
      uses: actions/upload-artifact@v3
      with:
        name: course-data-${{ steps.scrape.outputs.date }}
        path: data/${{ steps.scrape.outputs.filename }}
        retention-days: 90