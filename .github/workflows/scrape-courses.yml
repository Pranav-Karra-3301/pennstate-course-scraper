name: Weekly Course Scraping

on:
  schedule:
    # Run every Sunday at 6 AM UTC (1 AM EST, 10 PM PST Sat)
    - cron: '0 6 * * 0'
  workflow_dispatch:  # Allow manual triggering
    inputs:
      test_mode:
        description: 'Run in test mode (limited subjects)'
        required: false
        default: 'false'
        type: boolean
      max_subjects:
        description: 'Maximum subjects to scrape (for testing)'
        required: false
        default: '10'
        type: string

jobs:
  scrape-courses:
    runs-on: ubuntu-latest
    timeout-minutes: 120  # Global timeout for the entire job
    permissions:
      contents: write
      pull-requests: write
    
    steps:
    - name: Checkout repository
      uses: actions/checkout@v4
      with:
        token: ${{ secrets.GITHUB_TOKEN }}
    
    - name: Set up Python
      uses: actions/setup-python@v5
      with:
        python-version: '3.11'
        cache: 'pip'
    
    - name: Install system dependencies
      run: |
        set -euo pipefail
        echo "ğŸ“¦ Installing system dependencies..."
        sudo apt-get update || echo "Warning: apt-get update failed, continuing..."
        sudo apt-get install -y bc jq || echo "Warning: Some packages failed to install"
        
        # Verify installations
        command -v bc >/dev/null 2>&1 || { echo "âŒ bc is not installed"; exit 1; }
        command -v jq >/dev/null 2>&1 || { echo "âŒ jq is not installed"; exit 1; }
        echo "âœ… System dependencies installed successfully"
    
    - name: Install Python dependencies
      run: |
        set -euo pipefail
        echo "ğŸ Installing Python dependencies..."
        
        # Upgrade pip first
        python -m pip install --upgrade pip
        
        # Install requirements with retry logic
        max_attempts=3
        attempt=1
        while [ $attempt -le $max_attempts ]; do
          echo "Attempt $attempt of $max_attempts..."
          if pip install -r requirements.txt --no-cache-dir; then
            echo "âœ… Python dependencies installed successfully"
            break
          else
            echo "âš ï¸ Installation attempt $attempt failed"
            if [ $attempt -eq $max_attempts ]; then
              echo "âŒ Failed to install Python dependencies after $max_attempts attempts"
              exit 1
            fi
            attempt=$((attempt + 1))
            sleep 5
          fi
        done
        
        # Verify key imports
        python -c "import requests, bs4, pandas, aiohttp" || { echo "âŒ Failed to import required modules"; exit 1; }
    
    - name: Pre-flight checks
      id: preflight
      run: |
        set -euo pipefail
        echo "ğŸ” Running pre-flight checks..."
        
        # Check if scraper exists
        if [ ! -f "scraper_optimized.py" ]; then
          echo "âŒ scraper_optimized.py not found!"
          exit 1
        fi
        
        # Check Python syntax
        python -m py_compile scraper_optimized.py || { echo "âŒ Python syntax error in scraper"; exit 1; }
        
        # Create necessary directories
        mkdir -p data logs artifacts
        
        # Test scraper import
        python -c "from scraper_optimized import OptimizedLionPathScraper" || { echo "âŒ Failed to import scraper"; exit 1; }
        
        echo "âœ… Pre-flight checks passed"
        echo "status=success" >> $GITHUB_OUTPUT
    
    - name: Run course scraper
      id: scrape
      timeout-minutes: 90  # Timeout for scraper execution
      run: |
        set -euo pipefail
        echo "ğŸš€ Starting course scrape at $(date)"
        
        # Initialize variables with defaults
        start_time=$(date +%s)
        datestamp=$(date +%Y%m%d)
        output_file="data/psu_courses_${datestamp}.jsonl"
        
        # Initialize statistics with defaults
        unique_courses=0
        total_sections=0
        subjects_processed=0
        total_subjects=0
        failed_subjects=0
        sections_rate=0
        courses_rate=0
        success_rate=0
        file_size=0
        line_count=0
        
        # Determine scraping parameters
        if [ "${{ inputs.test_mode }}" = "true" ]; then
          echo "ğŸ§ª Running in test mode with ${{ inputs.max_subjects }} subjects"
          max_subjects_arg="--max-subjects ${{ inputs.max_subjects }}"
          workers=5
          detail_workers=10
          min_courses=5  # Minimum expected courses in test mode
          min_sections=10  # Minimum expected sections in test mode
        else
          echo "ğŸƒ Running full scrape"
          max_subjects_arg=""
          workers=12
          detail_workers=40
          min_courses=100  # Minimum expected courses in production
          min_sections=500  # Minimum expected sections in production
        fi
        
        # Run the scraper with retry logic
        max_attempts=2
        attempt=1
        scrape_success=false
        
        while [ $attempt -le $max_attempts ] && [ "$scrape_success" = "false" ]; do
          echo "ğŸ“Š Scraping attempt $attempt of $max_attempts..."
          
          if timeout 5400 python scraper_optimized.py \
            --output "${output_file}" \
            --format jsonl \
            --campus UP \
            --max-workers ${workers} \
            --max-detail-workers ${detail_workers} \
            --rate-limit 15 \
            --retry-attempts 3 \
            ${max_subjects_arg} 2>&1 | tee "logs/scrape_output_attempt_${attempt}.log"; then
            
            scrape_success=true
            echo "âœ… Scraper execution completed"
            cp "logs/scrape_output_attempt_${attempt}.log" scrape_output.log
          else
            echo "âš ï¸ Scraper attempt $attempt failed with exit code $?"
            attempt=$((attempt + 1))
            if [ $attempt -le $max_attempts ]; then
              echo "ğŸ”„ Retrying in 30 seconds..."
              sleep 30
            fi
          fi
        done
        
        if [ "$scrape_success" = "false" ]; then
          echo "âŒ Scraper failed after $max_attempts attempts"
          exit 1
        fi
        
        # Calculate duration
        end_time=$(date +%s)
        duration=$((end_time - start_time))
        
        # Extract statistics from log with robust error handling
        echo "ğŸ“Š Extracting statistics from log..."
        
        # Helper function to safely extract numbers
        extract_number() {
          local pattern="$1"
          local default="$2"
          local value=$(grep -o "$pattern" scrape_output.log 2>/dev/null | grep -o "[0-9]*" | tail -1 || echo "$default")
          echo "${value:-$default}"
        }
        
        # Extract statistics with fallbacks
        unique_courses=$(extract_number "Unique courses: [0-9]*" "0")
        total_sections=$(extract_number "Total sections: [0-9]*" "0")
        
        # Extract subjects processed (format: "Subjects processed: X/Y")
        subjects_line=$(grep "Subjects processed:" scrape_output.log 2>/dev/null | tail -1 || echo "")
        if [ -n "$subjects_line" ]; then
          subjects_processed=$(echo "$subjects_line" | sed -n 's/.*Subjects processed: \([0-9]*\).*/\1/p' || echo "0")
          total_subjects=$(echo "$subjects_line" | sed -n 's/.*Subjects processed: [0-9]*\/\([0-9]*\).*/\1/p' || echo "0")
        fi
        
        failed_subjects=$(extract_number "Failed subjects: [0-9]*" "0")
        
        # Calculate rates with division by zero protection
        if [ "$duration" -gt 0 ]; then
          if [ "$total_sections" -gt 0 ]; then
            sections_rate=$(echo "scale=2; $total_sections / $duration" | bc -l 2>/dev/null || echo "0")
          fi
          if [ "$unique_courses" -gt 0 ]; then
            courses_rate=$(echo "scale=2; $unique_courses / $duration" | bc -l 2>/dev/null || echo "0")
          fi
        fi
        
        # Calculate success rate with protection
        if [ "$total_subjects" -gt 0 ] && [ "$subjects_processed" -gt 0 ]; then
          successful_subjects=$((subjects_processed - failed_subjects))
          if [ "$successful_subjects" -lt 0 ]; then
            successful_subjects=0
          fi
          success_rate=$(echo "scale=1; $successful_subjects * 100 / $total_subjects" | bc -l 2>/dev/null || echo "0")
        fi
        
        # Verify output file exists and get metrics
        if [ -f "${output_file}" ]; then
          file_size=$(stat -c%s "${output_file}" 2>/dev/null || wc -c < "${output_file}" 2>/dev/null || echo "0")
          line_count=$(wc -l < "${output_file}" 2>/dev/null || echo "0")
          
          echo "âœ… Output file created: ${output_file}"
          echo "ğŸ“Š File size: ${file_size} bytes, Lines: ${line_count}"
          
          # Validate minimum data thresholds
          if [ "$line_count" -lt "$min_courses" ]; then
            echo "âš ï¸ Warning: Only ${line_count} courses found, expected at least ${min_courses}"
            if [ "${{ inputs.test_mode }}" != "true" ]; then
              echo "âŒ Production run produced insufficient data"
              exit 1
            fi
          fi
        else
          echo "âŒ Output file not found: ${output_file}"
          exit 1
        fi
        
        # Store outputs for later steps (with defaults to prevent failures)
        {
          echo "duration=${duration:-0}"
          echo "unique_courses=${unique_courses:-0}"
          echo "total_sections=${total_sections:-0}"
          echo "subjects_processed=${subjects_processed:-0}"
          echo "total_subjects=${total_subjects:-0}"
          echo "failed_subjects=${failed_subjects:-0}"
          echo "sections_rate=${sections_rate:-0}"
          echo "courses_rate=${courses_rate:-0}"
          echo "success_rate=${success_rate:-0}"
          echo "filename=psu_courses_${datestamp}.jsonl"
          echo "date=$(date +%Y-%m-%d)"
          echo "datetime=$(date)"
          echo "file_size=${file_size:-0}"
          echo "line_count=${line_count:-0}"
          echo "scrape_success=true"
        } >> $GITHUB_OUTPUT
        
        # Summary
        echo "============================================"
        echo "ğŸ“Š SCRAPING SUMMARY"
        echo "============================================"
        echo "â±ï¸  Duration: $duration seconds"
        echo "ğŸ“ Unique courses: $unique_courses"
        echo "ğŸ“š Total sections: $total_sections"  
        echo "ğŸ“– Subjects: $subjects_processed/$total_subjects"
        echo "âœ… Success rate: ${success_rate}%"
        echo "âš¡ Processing rate: $sections_rate sections/sec"
        echo "ğŸ’¾ Output: ${line_count} lines, ${file_size} bytes"
        echo "============================================"
    
    - name: Validate output data
      id: validate
      if: steps.scrape.outputs.scrape_success == 'true'
      run: |
        set -euo pipefail
        output_file="data/${{ steps.scrape.outputs.filename }}"
        
        echo "ğŸ” Validating output file: $output_file"
        
        if [ ! -f "$output_file" ]; then
          echo "âŒ Output file missing: $output_file"
          exit 1
        fi
        
        # Check file is not empty
        if [ ! -s "$output_file" ]; then
          echo "âŒ Output file is empty"
          exit 1
        fi
        
        # Validate JSON structure with detailed error reporting
        echo "ğŸ“‹ Validating JSONL format..."
        
        # Create temporary Python validation script using printf to avoid heredoc issues
        printf '%s\n' \
          'import json' \
          'import sys' \
          '' \
          'def validate_jsonl(filepath):' \
          '    try:' \
          '        with open(filepath, "r") as f:' \
          '            lines = f.readlines()' \
          '        ' \
          '        if not lines:' \
          '            print("âŒ File is empty")' \
          '            sys.exit(1)' \
          '        ' \
          '        valid_lines = 0' \
          '        invalid_lines = []' \
          '        ' \
          '        for i, line in enumerate(lines, 1):' \
          '            try:' \
          '                data = json.loads(line.strip())' \
          '                if "course" in data and "sections" in data:' \
          '                    valid_lines += 1' \
          '                else:' \
          '                    invalid_lines.append(i)' \
          '            except json.JSONDecodeError:' \
          '                invalid_lines.append(i)' \
          '        ' \
          '        print(f"ğŸ“Š Validation Results:")' \
          '        print(f"  Total lines: {len(lines)}")' \
          '        print(f"  Valid records: {valid_lines}")' \
          '        print(f"  Invalid records: {len(invalid_lines)}")' \
          '        ' \
          '        if invalid_lines:' \
          '            print(f"âŒ Invalid lines: {invalid_lines[:5]}")' \
          '            sys.exit(1)' \
          '        ' \
          '        if valid_lines == len(lines):' \
          '            print(f"âœ… All {valid_lines} records are valid!")' \
          '            first = json.loads(lines[0])' \
          '            c = first.get("course", {})' \
          '            print(f"ğŸ“š Sample: {c.get('"'"'course_code'"'"', '"'"'N/A'"'"')} - {c.get('"'"'course_title'"'"', '"'"'N/A'"'"')}")' \
          '        else:' \
          '            rate = (valid_lines / len(lines)) * 100' \
          '            if rate < 95:' \
          '                print(f"âš ï¸ Only {rate:.1f}% records valid")' \
          '                sys.exit(1)' \
          '    except Exception as e:' \
          '        print(f"ğŸ’¥ Validation error: {e}")' \
          '        sys.exit(1)' \
          '' \
          'if __name__ == "__main__":' \
          '    if len(sys.argv) != 2:' \
          '        print("Usage: python validate_jsonl.py <file>")' \
          '        sys.exit(1)' \
          '    validate_jsonl(sys.argv[1])' \
          > /tmp/validate_jsonl.py
        
        # Execute the validation script
        python3 /tmp/validate_jsonl.py "$output_file"
        rm -f /tmp/validate_jsonl.py
        
        echo "validation_complete=true" >> $GITHUB_OUTPUT
        echo "âœ… Output validation completed successfully"
    
    - name: Create summary report
      if: steps.validate.outputs.validation_complete == 'true'
      run: |
        set -euo pipefail
        
        # Create comprehensive summary report using echo to avoid YAML parsing issues
        {
          echo "# Course Scraping Summary - ${{ steps.scrape.outputs.date }}"
          echo ""
          echo "## ğŸ“Š Statistics"
          echo ""
          echo "â€¢ **Date**: ${{ steps.scrape.outputs.datetime }}"
          echo "â€¢ **Duration**: ${{ steps.scrape.outputs.duration }} seconds"
          echo "â€¢ **Unique Courses**: ${{ steps.scrape.outputs.unique_courses }}"
          echo "â€¢ **Total Sections**: ${{ steps.scrape.outputs.total_sections }}"
          echo "â€¢ **Subjects Processed**: ${{ steps.scrape.outputs.subjects_processed }}/${{ steps.scrape.outputs.total_subjects }}"
          echo "â€¢ **Failed Subjects**: ${{ steps.scrape.outputs.failed_subjects }}"
          echo "â€¢ **Success Rate**: ${{ steps.scrape.outputs.success_rate }}%"
          echo "â€¢ **Processing Rate**: ${{ steps.scrape.outputs.sections_rate }} sections/second"
          echo ""
          echo "## ğŸ“ Output File"
          echo ""
          echo "â€¢ **Filename**: \`data/${{ steps.scrape.outputs.filename }}\`"
          echo "â€¢ **Format**: JSONL (optimized structure)"
          echo "â€¢ **Size**: ${{ steps.scrape.outputs.file_size }} bytes"
          echo "â€¢ **Records**: ${{ steps.scrape.outputs.line_count }} course records"
          echo "â€¢ **Campus**: University Park (UP)"
          echo "â€¢ **Semester**: Fall 2025"
          echo ""
          echo "## ğŸ” Data Quality"
          echo ""
          echo "â€¢ **Structure**: Optimized format separating course-level from section-level data"
          echo "â€¢ **Validation**: All records validated as proper JSON with required fields"
          echo "â€¢ **Completeness**: Comprehensive course and section details included"
          echo ""
          echo "## ğŸ“ Run Details"
          echo ""
          echo "â€¢ **Trigger**: ${{ github.event_name == 'schedule' && 'Weekly schedule (Sundays 6 AM UTC)' || 'Manual trigger' }}"
          echo "â€¢ **Test Mode**: ${{ inputs.test_mode == 'true' && 'Yes' || 'No' }}"
          echo "â€¢ **Environment**: Ubuntu Latest (GitHub Actions)"
          echo "â€¢ **Python Version**: 3.11"
          echo ""
          echo "## ğŸ¤– Automation"
          echo ""
          echo "This automated scraping run was executed with:"
          echo "â€¢ Respectful rate limiting (15 requests/second)"
          echo "â€¢ Comprehensive error handling and retries"
          echo "â€¢ Optimized data structure to reduce redundancy"
          echo "â€¢ Focus on University Park campus courses"
          echo ""
          echo "---"
          echo ""
          echo "Generated automatically by [Penn State Course Scraper](https://github.com/${{ github.repository }})"
        } > scrape_summary.md
        
        echo "âœ… Summary report created"
    
    - name: Configure Git
      if: steps.validate.outputs.validation_complete == 'true'
      run: |
        set -euo pipefail
        
        # Configure git with error handling
        git config --local user.email "action@github.com" || {
          echo "âš ï¸ Failed to set git email, trying global config"
          git config --global user.email "action@github.com"
        }
        
        git config --local user.name "GitHub Action Bot" || {
          echo "âš ï¸ Failed to set git name, trying global config"
          git config --global user.name "GitHub Action Bot"
        }
        
        # Verify configuration
        git config --get user.email || { echo "âŒ Git email not configured"; exit 1; }
        git config --get user.name || { echo "âŒ Git name not configured"; exit 1; }
        
        echo "âœ… Git configured successfully"
    
    - name: Create Pull Request
      if: steps.validate.outputs.validation_complete == 'true'
      id: create_pr
      uses: peter-evans/create-pull-request@v5
      with:
        token: ${{ secrets.GITHUB_TOKEN }}
        commit-message: |
          ğŸ“Š Course data update - ${{ steps.scrape.outputs.date }}
          
          - Unique courses: ${{ steps.scrape.outputs.unique_courses }}
          - Total sections: ${{ steps.scrape.outputs.total_sections }}
          - Processing time: ${{ steps.scrape.outputs.duration }}s
          - Success rate: ${{ steps.scrape.outputs.success_rate }}%
        title: "ğŸ“Š Course Data Update - ${{ steps.scrape.outputs.date }}"
        body: |
          ## ğŸ“ Penn State Course Data Update
          
          This automated pull request contains fresh course data scraped from Penn State LionPath.
          
          ### ğŸ“ˆ Performance Metrics
          
          | Metric | Value |
          |--------|-------|
          | **Unique Courses** | ${{ steps.scrape.outputs.unique_courses }} |
          | **Total Sections** | ${{ steps.scrape.outputs.total_sections }} |
          | **Processing Time** | ${{ steps.scrape.outputs.duration }}s |
          | **Processing Rate** | ${{ steps.scrape.outputs.courses_rate }} courses/sec |
          | **Success Rate** | ${{ steps.scrape.outputs.success_rate }}% |
          | **File Size** | ${{ steps.scrape.outputs.file_size }} bytes |
          
          ### ğŸ“ Files Updated
          
          - `data/${{ steps.scrape.outputs.filename }}` - Course data in optimized JSONL format
          - `scrape_summary.md` - Detailed summary report
          
          ### ğŸ”§ Data Structure
          
          Uses optimized structure with:
          - 50-70% smaller file sizes
          - Logical separation of course/section data
          - Built-in statistics and aggregation
          
          ### ğŸ¤– Automation Details
          
          - **Trigger**: ${{ github.event_name == 'schedule' && 'Weekly automated run' || 'Manual execution' }}
          - **Test Mode**: ${{ inputs.test_mode == 'true' && 'Yes' || 'No' }}
          - **Rate Limiting**: 15 requests/second
          - **Error Handling**: Comprehensive retry logic
          
          ---
          
          **Note**: This is an automated pull request. Please review the changes before merging.
        branch: course-update-${{ steps.scrape.outputs.date }}
        delete-branch: true
        labels: |
          automated
          data-update
          course-scraping
    
    - name: Upload logs artifact
      if: always()
      uses: actions/upload-artifact@v4
      with:
        name: scraping-logs-${{ steps.scrape.outputs.date || github.run_id }}
        path: |
          scrape_output.log
          logs/*.log
          psu_scraper_optimized.log
        retention-days: 30
        if-no-files-found: warn
    
    - name: Upload data artifact
      if: steps.validate.outputs.validation_complete == 'true'
      uses: actions/upload-artifact@v4
      with:
        name: course-data-${{ steps.scrape.outputs.date }}
        path: |
          data/${{ steps.scrape.outputs.filename }}
          scrape_summary.md
        retention-days: 90
        if-no-files-found: error
    
    - name: Report failure
      if: failure()
      run: |
        echo "âŒ Workflow failed!"
        echo "Debug information:"
        echo "- Event: ${{ github.event_name }}"
        echo "- Test mode: ${{ inputs.test_mode }}"
        echo "- Run ID: ${{ github.run_id }}"
        echo "- Run number: ${{ github.run_number }}"
        
        # Create failure summary for GitHub Actions
        {
          echo "## âŒ Course Scraping Failed"
          echo ""
          echo "The course scraping workflow encountered an error."
          echo ""
          echo "### Debug Information"
          echo "â€¢ **Run ID**: ${{ github.run_id }}"
          echo "â€¢ **Run Number**: ${{ github.run_number }}"
          echo "â€¢ **Trigger**: ${{ github.event_name }}"
          echo "â€¢ **Test Mode**: ${{ inputs.test_mode }}"
          echo ""
          echo "Please check the logs for more details."
        } >> $GITHUB_STEP_SUMMARY
    
    - name: Success notification
      if: success() && steps.create_pr.outputs.pull-request-number
      run: |
        echo "âœ… Workflow completed successfully!"
        echo "ğŸ“‹ Pull Request: #${{ steps.create_pr.outputs.pull-request-number }}"
        
        # Create success summary for GitHub Actions
        {
          echo "## âœ… Course Scraping Successful"
          echo ""
          echo "Successfully scraped Penn State course data."
          echo ""
          echo "### ğŸ“Š Results"
          echo "â€¢ **Courses**: ${{ steps.scrape.outputs.unique_courses }}"
          echo "â€¢ **Sections**: ${{ steps.scrape.outputs.total_sections }}"
          echo "â€¢ **Success Rate**: ${{ steps.scrape.outputs.success_rate }}%"
          echo "â€¢ **Pull Request**: #${{ steps.create_pr.outputs.pull-request-number }}"
          echo ""
          echo "### ğŸ“ Output"
          echo "â€¢ **File**: \`data/${{ steps.scrape.outputs.filename }}\`"
          echo "â€¢ **Size**: ${{ steps.scrape.outputs.file_size }} bytes"
          echo "â€¢ **Records**: ${{ steps.scrape.outputs.line_count }}"
        } >> $GITHUB_STEP_SUMMARY